% Fundamentação Teórica
\chapter{Fundamentação Teórica}

Este é o primeiro capítulo da parte central do trabalho, isto é, o desenvolvimento, a parte mais extensa de todo o trabalho. Geralmente o desenvolvimento é dividido em capítulos, cada um com subseções e subseções, cujo tamanho e número de divisões variam em função da natureza do conteúdo do trabalho.

Em geral, a parte de desenvolvimento é subdividida em quatro subpartes:
\begin{itemize}
	\item \textit{contextualização ou definição do problema} -- consiste em descrever a situação ou o contexto geral referente ao assunto em questão, devem constar informações atualizadas visando a proporcionar maior consistência ao trabalho;
	\item \textit{referencial ou embasamento teórico} -- texto no qual se deve apresentar os aspectos teóricos, isto é, os conceitos utilizados e a definição dos mesmos; nesta parte faz-se a revisão de literatura sobre o assunto, resumindo-se os resultados de estudos feitos por outros autores, cujas obras citadas e consultadas devem constar nas referências;
	\item \textit{metodologia do trabalho ou procedimentos metodológicos} -- deve constar o instrumental, os métodos e as técnicas aplicados para a elaboração do trabalho;
	\item \textit{resultados} -- devem ser apresentados, de forma objetiva, precisa e clara, tanto os resultados positivos quanto os negativos que foram obtidos com o desenvolvimento do trabalho, sendo feita uma discussão que consiste na avaliação circunstanciada, na qual se estabelecem relações, deduções e generalizações.
\end{itemize}

É recomendável que o número total de páginas referente à parte de desenvolvimento não ultrapasse 60 (sessenta) páginas.

\section{Cidades Inteligentes}


\section{Plataformas de processamento em tempo real}

As plataformas de processamento em tempo real \abrv[RTC --­ Real­Time Computing){RTC (Real­Time Computing)}, compostas por hardware ou software, são sistemas responsáveis por emitir respostas a um dado evento dentro de um determinado deadline (limite de tempo). Sendo tal limite de tempo, normalmente na ordem de mile / micro­segundos [82, 84].

\section{Apache Spark}

Apache Spark \cite{Spark} é uma plataforma de computação em cluster (unidade lógica composta por um conjunto de computadores conectados entre si através da Rede, permitindo compartilhamento de recursos \cite{Techopedia}), com suporte a consultas a banco de dados, processamento de stream e aprendizado de máquina. Tendo Java, Python e Scala como linguagens de programação suportadas.

A arquitetura do Spark, conforme a Fig. \ref{fig:sparkStack}, é composta por uma pilha integrando os seguintes componentes, a serem explicados posteriormente: Spark SQL, Spark Streaming, MLib, GraphX, Spark Core e Administradores de Cluster (Yarn \cite{Yarn} e Apache Mesos \cite{Mesos}). Tal estrutura visa ser de fácil manutenção, teste, deploy, e permitir aumento de perfomance do núcleo impactando seus demais componentes.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/sparkStackImage.png}
  \caption{Ilustração da arquitetura em pilha do Apache Spark}
  {Fonte:} \cite{LearningSpark}
  \label{fig:sparkStack}
\end{figure}

\subsection{Módulos do Apache Spark}

O Spark Core é o principal módulo do Apache Spark, responsável principalmente pelo gerenciamento de memória,  tasks (conceito explicado em \ref{taskSection}) e tolerância a falha. Ainda nele, a abstração conhecida como  \abrv[RDD -- Resilient Distributed Dataset]{RDD (Resilient Distributed Dataset)} é definida, cujo papel é o de representar uma coleção de dados distribuídos e manipulados em paralelo. Os RDDs em Java são representados pela classe JavaRDD, criados através da classe JavaSparkContext (contexto da aplicação), que é instanciada recebendo como parâmetro um objeto SparkConf, contendo informações relacionadas ao nome da aplicação e o endereço em que ela será executada, pondendo ser local, ou, em cluster.

Outro módulo é o Spark Streaming, o qual realiza o processamento de dados em stream. Os streams são representados por uma sequência de RDDs, conhecida como \abrv[DStream -- Discretized Stream]{DStream}. O contexto de um streaming (JavaStreamingContext) é criado usando as configurações da aplicação e o intervalo no qual cada DStream será definido. Após isso, tais streams são atribuídos a um objeto da classe JavaReceiverInputDStream.

Além dos módulos detalhados nos parágrafos anteriores, é relevante mencionar o (i) Spark SQL, responsável por trabalhar com dados estruturados; o (ii) MLib, relacionado ao aprendizado de máquina, contendo algorítimos tais como o de classificação, regressão, "clusterização", filtragem colaborativa, avaliação de modelo e importação de dados; e o (ii) GraphX, que é composto por uma biblioteca para manipulação de grafos e computações paralelas. Por fim, o Spark também possui um administrador de cluster padrão, conhecido como Standalone Scheduler \cite{Spark}, tendo também suporte ao YARN e Apache Mesos.

\subsection{Composição Interna do Apache Spark}
\label{sparkInternalsSection}

Nesta seção é explicada a composição interna do Apache Spark, expondo os componentes que permitem a execução distribuída de uma aplicação spark, tais como o Driver Program, Spark Context, Worker Node e Executor.
Também são explicados os conceitos sobre as operações de Transformação e Ação, e os relativos a Job, Stage, Task e Partição RDD.

\subsubsection{Modelo de execução das aplicações Spark}
\label{sparkModelSection}

O modelo de execução das aplicações Spark é definido pela relação composta por um driver, SparkContext, executors e tasks, conforme ilustrado na Fig. \ref{fig:sparkComponents}. Nessa interação, as aplicações Spark funcionam num Worker Node (qualquer nó capaz de de executar código de aplicação localmente ou num cluster) como uma espécie de Driver, o qual executa operações paralelas e define dados distribuídos sobre um cluster. Além disso, o driver é responsável por encapsular a função principal da aplicação e prover acesso ao Spark, utilizando o objeto da classe SparkContext. Tal objeto é usado também para representar uma conexão com um cluster e construir RDDs \cite{LearningSpark}. 

Após a construção de um RDD, é possível executar operações sobre ele. Essas operações são realizadas por executors, processos administrados por uma aplicação Spark (cada aplicação tem os seus próprios executors); nos quais há espaços reservados para execução de tasks (conceito explicado na seção \ref{taskSection}). Há dois tipos de operações: (i) Ações (actions) e (ii) Transformações (transformations) \cite{Spark}. O primeiro tipo retorna um resultado ao driver após computações sobre um conjunto de dados de acordo com uma função. Sendo o segundo, responsável por gerar novos conjuntos de dados (RDDs) através, também, de funções especificadas \cite{LearningSpark}. 

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/sparkComponentsImage.png}
  \caption{Componentes do Spark para execução distribuída}
  {Fonte:} \cite{LearningSpark}
  \label{fig:sparkComponents}
\end{figure}

\subsubsection{Job}
\label{jobSection}

A abstração do conceito de  job está relacionada com o de action. Mais especificamente, um job é o conjunto de estágios (stages) resultantes de uma
determinada  action. Por exemplo, quando o método count() é invocado (para realizar a contagem de elementos dentro de um RDD), cria­-se um job composto por um ou mais estágios. Os jobs, por padrão, são escalonados para serem executados em ordem \abrv[FIFO -- First In, First Out]{FIFO (First In, First Out)}. Além disso, o scheduler do Spark é responsável por criar um plano de execução física, o qual é utilizado para calcular as RDDs necessárias para executar a ação invocada \cite{LearningSpark}, \cite{SparkInternals}.

Em tal plano de execução física, defini­-se basicamente um grafo acíclico dirigido \abrv[DAG -- Directed Acyclic Graph]{DAG (Directed Acyclic Graph \cite{GraphTeory})}, relacionando as dependências existentes entre os RDDs, numa espécie de linhagem de execução (lineage). Após isso, a partir da última RDD do último estágio, cada dependência é calculada, de trás para frente, para que posteriormente os RDDs dependentes possam ser calculados, até que todas as actions necessárias tenham terminado \cite{LearningSpark}, \cite{SparkInternals}.

\subsubsection{Stage}
\label{stageSection}

A divisão de um job resulta no conceito de stage, ou seja, jobs são compostos por um ou mais stages, os quais têm  tasks a serem executadas. Nem sempre a relação entre  stages e RDDs será de 1:1. No mais simples dos casos, para cada RDD há um stage, sendo possível nos mais complexos haver mais de um stage gerado por um único RDD \cite{LearningSpark}, \cite{SparkInternals}.

Por exemplo, havendo três RDDs no grafo RDD, não necessariamente haverão três stages. Um dos motivos para isso acontecer é quando ocorre pipelining, situação na qual é possível reduzir o número de  stages, calculando os valores de alguns  RDDs  utilizando unicamente informac?o?es de seus antecessores, sem movimentação de dados de outros RDDs \cite{LearningSpark}, \cite{SparkInternals}.

Além da possibilidade do número de stages ser reduzido, o contrário acontece quando fronteiras entre  stages são definidas. Tal situação ocorre porque algumas transformações, como a reduceByKey (função de agregação por chaves), podem resultar em reparticionamento do conjunto de dados para a computação de alguma saída, exigindo dados de outros RDDs para a formação de um único RDD. Assim, em cada fronteira entre os  stages, tasks do estágio antecessor são responsáveis por escrever dados para o disco e buscar tasks no estágio sucessor, através da rede, resultando em um processo custoso, conhecido como shuffling \cite{LearningSpark}, \cite{SparkInternals}.

O número de partições entre o estágio predecessor e sucessor podem ser diferentes, sendo possível customizá-lo, embora isso não seja recomendável. Tal aconselhamento é devido ao fato de que se o número for modificado para uma pequena quantidade de partições, tais podem sofrer com tasks overloaded (sobrecarregadas), do contrário, havendo partições em excesso, resultará em um alto número de shuffling entre elas \cite{LearningSpark}, \cite{SparkInternals}.

Em resumo, shuffling sempre acontecerá quando for necessário obter dados de outras partições para computar um resultado, sendo esse um procedimento custoso devido ao número de operações de entrada e saída envolvendo: leitura/escrita em disco e transferência de dados através da rede. No entanto, se necessário, o Spark provê uma forma eficiente para reparticionamento, conhecida como coalesce(), a qual reduz o número de partições sem causar movimentação de dados \cite{LearningSpark}.

\subsubsection{Task}
\label{taskSection}

A abstração do conceito task  está relacionada principalmente com stage, pois, uma vez que os estágios são definidos, tasks são criadas e enviadas para um scheduler interno. Além disso, tasks estão muito próximas da definição de partition, pelo fato de ser necessária a existe?ncia de uma task para cada partition, para executar as computações necessárias sobre ela \cite{LearningSpark}.

Portanto, o número de tasks em um stage é definido pelo número de partições existentes no RDD antecessor. E, por outro lado, o número de partições em um determinado RDD é igual ao número de partições existentes no RDD do qual ele depende. No entanto, há exceções em caso de (i) coalescing, processo o qual permite criar um RDD com menos partições que seu antecessor; (ii) união, sendo o número de partições resultado da soma do número de partições predecessoras; (iii) produto cartesiano, produto dos predecessores \cite{LearningSpark}.

Cada  task internamente é divida em algumas etapas, sendo elas: (i) carregamento dos dados de entrada, sejam eles provenientes de unidades de armazenamento (caso o RDD seja um RDD de entrada de dados), de um RDD existente (armazenado em memória cache), ou, de saídas de outro RDD (processo de shuffling), (ii) processamento da operação necessária para computação do RDD representado por ela, (iii) escrever a saída para o processo de shuffling, para um armazenamento externo ou para o driver (caso seja o RDD final de uma ação). Em resumo, uma  task é responsável por coletar os dados de entrada, processá-­los e, por fim, gerar uma saída \cite{LearningSpark}.

\section{Apache Storm}

Apache Storm \cite{Storm} é uma plataforma de computação em cluster, com suporte a processamento de stream  de dados em tempo real. O Storm requer pouca manutenção, é de fácil de administração e tem suporte a qualquer linguagem de programação (que leia e escreva fluxos de dados padronizados), posto que em seu núcleo é utilizado o Apache Thrift \cite{Thrift} (framework que permite o desenvolvimento de serviços multilinguagem) para definir e submeter topologias (conceito definido em \ref{topologySection}) \cite{Storm}.

A arquitetura do Storm, conforme a Fig. \ref{fig:stormComponents}, é composta pelos seguintes componentes, a serem explicados posteriormente: Nimbus, ZooKeeper e Supervisor Node.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/stormComponentsImage.png}
  \caption{Ilustração da arquitetura do Apache Storm}
  {Fonte:} \cite{LearningStorm}
  \label{fig:stormComponents}
\end{figure}

\subsection{Composição interna do Apache Storm}

\subsubsection{Nimbus}
\label{nimbusSection}

Nimbus é o processo master executado num cluster Storm, tendo ele uma única instância e sendo responsável por distribuir o código da aplicação (job) para os nós workers (computadores que compõem o cluster), aos quais são atribuídas tasks (parte de um job). As tasks são monitoradas pelo Supervisor node, sendo reiniciadas em caso de falha e quando requisitado. Ainda, caso um Supervisor node falhe continuadamente, o job é atribuído para outro nó worker \cite{LearningStorm}, \cite{StormPython}. 

O Nimbus utiliza um protocolo de comunicação sem estado (Stateless protocol), ou seja, cada requisição é tratada individualmente, sem relação com a anterior, não armazenando dados ou estado \cite{Techopedia}, sendo assim todos os seus dados são armazenados no ZooKeeper (definido em \ref{zookeeperSection}). Além disso ele é projetado para ser fail­-fast, ou seja, em caso de falha é rapidamente reiniciado, sem afetar as  tasks em execução nos nós  workers \cite{LearningStorm}, \cite{StormPython}.

\subsubsection{Supervisor node}
\label{supervisorSection}

Cada Supervisor node é responsável pela administração de um determinado nó do cluster Storm; gerenciando o ciclo de vida de processos workers relacionados a execução das tasks (partes de uma topologia) atribuídas a ele próprio. Os workers quando em execução emitem "sinais de vida" (heartbeats), possibilitando o supervisor detectar e reiniciar caso não estejam respondendo. E, assim como o Nimbus, um Supervisor node é projetado para ser  fail­-fast \cite{LearningStorm}, \cite{StormPython}.

\subsubsection{ZooKeeper}
\label{zookeeperSection}

O ZooKeeper Cluster \cite{ZooKeeper} é responsável por coordenar processos, informações compartilhadas, tasks submetidas ao Storm e estados associados ao cluster, num contexto distribuído e de forma confiável, sendo possível aumentar o nível de confiabilidade tendo mais de um servidor ZooKeeper. O ZooKeeper atua também como o intermediário da comunicação entre Nimbus e os Supervisor nodes, pois eles não se comunicam diretamente entre si, ambos também podem ser finalizados sem afetar o cluster, visto que todos os seus dados, são armazenados no ZooKeeper, como já mencionado anteriormente \cite{LearningStorm}, \cite{StormPython}.

\subsubsection{Topologia}
\label{topologySection}

A topologia Storm, ilustrada na Fig. \ref{fig:stormTopology}, é uma abstração que define um grafo acíclico dirigido \abrv[DAG -- Directed Acyclic Graph]{DAG (Directed Acyclic Graph \cite{GraphTeory})}, utilizado para computações de streams. Cada nó desse grafo é responsável por executar algum processamento, enviando seu resultado para o próximo nó do fluxo. Após definida a topologia, ela pode ser executada localmente ou submetida a um cluster \cite{LearningStorm}, \cite{StormPython}.

Stream é um dos componentes da topologia Storm, definido como uma sequência de tuplas independentes, fornecidas por um ou mais spout e processadas por um ou mais bolt. Cada stream tem o seu respectivo ID, que é utilizado pelos bolts para consumirem e produzirem as tuplas. As tuplas compõem uma unidade básica de dados, suportando os tipos de dados (serializáveis) no qual a topologia está sendo desenvolvida.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/stormTopologyImage.png}
  \caption{Exemplo de uma topologia do Storm}
  {Fonte:} \cite{LearningStorm}
  \label{fig:stormTopology}
\end{figure}

\paragraph{Bolt.} Os bolts são unidades de processamento de dados (streams), sendo eles responsáveis por executar transformações simples sobre as tuplas, as quais são combinadas com outras formando complexas transformações. Também, eles podem ser inscritos para receberem informações tanto de outros bolts, quanto dos spouts, além de produzirem streams como saída \cite{LearningStorm}.

Tais streams são declarados, emitidos para outro bolt e processados, respectivamente, pelos métodos declareStream, emit e execute. As tuplas não precisam ser processadas imediatamente quando chegam a um bolt, pois, talvez haja a necessidade de aguardar para executar alguma operação de junção (join) com outra tupla, por exemplo. Também, inúmeros métodos definidos pela interface Tuple podem recuperar meta­dados associados com a tupla recebida via execute \cite{LearningStorm}.

Por exemplo, se um ID de mensagem está associado com uma tupla, o método execute deverá publicar um evento ack, ou, fail, para o bolt, caso contrário o Storm não tem como saber se uma tupla foi processada. Sendo assim, o bolt envia automaticamente uma mensagem de confirmação após o término da execução do método execute e, em caso de um evento de falha, é lançada uma exceção \cite{LearningStorm}.

Além disso, num contexto distribuído, a topologia pode ser serializada e submetida, via Nimbus, para um cluster. No qual, será processada por worker nodes. Nesse contexto, o método prepare pode ser utilizado para assegurar que o bolt está, após a desserialização, configurado corretamente para executar as tuplas \cite{LearningStorm}, \cite{StormPython}.

\paragraph{Spout.} Na topologia Storm um  Spout é o responsável pelo fornecimento de tuplas, as quais são lidas e escritas por ele utilizando uma fonte externa de dados \cite{LearningStorm}. 

As tuplas emitidas por um  spout são rastreadas pelo Storm até terminarem o processamento, sendo emitida uma confirmação ao término dele. Tal procedimento somente ocorre se um ID de mensagem foi gerado na emissão da tupla e, caso esse ID tenha sido definido como nulo, o rastreamento não irá acontecer \cite{LearningStorm}.

Outra opção é definir um  time­out a topologia, sendo dessa forma enviada uma mensagem de falha para o  spout,  caso a tupla não seja processada dentro do tempo estipulado. Sendo necessário, novamente, definir um ID de mensagem. O custo dessa confirmação do processamento ter sido executado com sucesso é a pequena perda de performance, que pode ser relevante em determinados contextos; sendo assim é possível ignorar a emissão de IDs de mensagens
\cite{LearningStorm}.

Os principais métodos de um spout são: o open(), executado quando o spout é inicializado, sendo nele definida a lógica para acesso a dados de fontes externas; o de declaração de stream (declareStream) e o de processamento de próxima tupla (nextTuple), que ocorre se houver confirmação de sucesso da tupla anterior (ack), sendo o método fail chamado pelo Storm caso isso não ocorra \cite{LearningStorm}.

Por fim, nenhum dos métodos utilizados para a construção de um  spout devem ser bloqueante, pois o Storm executa todos os métodos numa mesma  thread. Além disso, todo  spout tem um buffer interno para o rastreamento dos status das tuplas emitidas, as quais são mantidas nele até uma confirmação de processamento concluído com sucesso (ack) ou falha (fail); não sendo inseridas novas tuplas (via nextTuple) no buffer quando ele esá cheio \cite{LearningStorm}.

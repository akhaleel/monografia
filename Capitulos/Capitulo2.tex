% Fundamentação Teórica
\chapter{Fundamentação Teórica}

Este é o primeiro capítulo da parte central do trabalho, isto é, o desenvolvimento, a parte mais extensa de todo o trabalho. Geralmente o desenvolvimento é dividido em capítulos, cada um com subseções e subseções, cujo tamanho e número de divisões variam em função da natureza do conteúdo do trabalho.

Em geral, a parte de desenvolvimento é subdividida em quatro subpartes:
\begin{itemize}
	\item \textit{contextualização ou definição do problema} -- consiste em descrever a situação ou o contexto geral referente ao assunto em questão, devem constar informações atualizadas visando a proporcionar maior consistência ao trabalho;
	\item \textit{referencial ou embasamento teórico} -- texto no qual se deve apresentar os aspectos teóricos, isto é, os conceitos utilizados e a definição dos mesmos; nesta parte faz-se a revisão de literatura sobre o assunto, resumindo-se os resultados de estudos feitos por outros autores, cujas obras citadas e consultadas devem constar nas referências;
	\item \textit{metodologia do trabalho ou procedimentos metodológicos} -- deve constar o instrumental, os métodos e as técnicas aplicados para a elaboração do trabalho;
	\item \textit{resultados} -- devem ser apresentados, de forma objetiva, precisa e clara, tanto os resultados positivos quanto os negativos que foram obtidos com o desenvolvimento do trabalho, sendo feita uma discussão que consiste na avaliação circunstanciada, na qual se estabelecem relações, deduções e generalizações.
\end{itemize}

É recomendável que o número total de páginas referente à parte de desenvolvimento não ultrapasse 60 (sessenta) páginas.

\section{Cidades Inteligentes}


\section{Plataformas de processamento em tempo real}

Nessa seção, é introduzido o conceito de plataformas de processamento em tempo real. Em seguida, a subseção \ref{espSubsection} tratará sobre processamento de fluxo de eventos em tempo real, e a \ref{metodologyOfChoice} a respeito da metodologia utilizada para selecionar as ferramentas com as quais o trabalho do Cap. \ref{chapter3} será realizado.

As plataformas de processamento em tempo real \abrv[RTC --­ Real­ Time Computing]{RTC (Real­ Time Computing)}, compostas por hardware ou software, são sistemas que tem como um dos principais requisitos emitir respostas a eventos de acordo com um determinado deadline (limite de tempo). Sendo assim, a corretude da computação não depende apenas da corretude lógica, mas também dos resultados serem produzidos de acordo com o deadline especificado \cite{RtcAsNewDiscipline}, \cite{Sommerville}.

Normalmente, os resultados são obtidos através de processamentos realizados por um conjunto de tasks (tarefas) cooperativas, iniciadas por eventos do sistema. Os eventos são dependentes do ambiente no qual estão inseridos, podendo ocorrer periodicamente (de forma regular e previsível), ou, aperiodicamente (irregulares e imprevisíveis) \cite{RtcAsNewDiscipline}, \cite{Sommerville}.

As tasks podem ter uma relação de interdependência e ao mesmo tempo nem todos os eventos que as originaram necessitarem de ser processados dentro de um deadline. Apesar disso, nenhuma task pode vir a comprometer o processamento de outra \cite{RtcAsNewDiscipline}.  

Com isso, os deadlines podem ser classificados em hard, firm, ou, soft. No primeiro caso, respectivamente, as respostas a todos os eventos devem necessariamente ocorrer dentro do  deadline  definido; no segundo, deadlines esporadicamente não atendidos são aceitos; no terceiro, deadlines não alcançados são permitidos frequentemente \cite{RtcAsNewDiscipline}. Além desses categorias, há os sistemas com delay (atraso) introduzido entre o intervalo de tempo de estímulo e resposta, os quais são classificados como near real time (NRT), ou seja, são sistemas de processamento em "quase tempo real".

Sendo assim, na categoria hard, é considerado como falha se o tempo estimado para um processamento não for atendido, e nas demais, a ocorrência disso resulta numa degradação (contínua de acordo com a quantidade de deadlines não atendidos) \cite{RtcAsNewDiscipline}, \cite{Sommerville}.

Entende-se por falha quando o usuário não recebe algo esperado (por exemplo, deadline não atendido) devido a um erro de sistema. Sendo esse erro um estado errôneo resultante de um defeito (característica do sistema que pode resultar em erro). E, degradação, como decréscimo da qualidade (conceito subjetivo, de acordo com os requisitos da aplicação) do sistema \cite{Sommerville}.

Por fim, é importante mencionar também que uma falha de sistema pode comprometer o requisito de Confiança e prejudicar um grande número de pessoas \cite{RtcAsNewDiscipline}, \cite{Sommerville}. Na Fig. \ref{fig:reliabilityDiagram} a seguir, são ilustradas as principais propriedades desse requisito:

\begin{figure}[htb]
  \centering
    \includegraphics[width=1\textwidth]{Imagens/reliabilityDiagram.png}
  \caption{Principais propriedades de Confiança}
  {Fonte:} \cite{Sommerville}
  \label{fig:reliabilityDiagram}
\end{figure}

\subsection{Processamento de fluxo de eventos em tempo real}
\label{espSubsection}

Fluxo (ou stream) de eventos, basicamente, pode ser entendido como uma série de eventos em função do tempo \cite{Datatorrent}, \cite{Complexevents}. Dentro desse escopo, duas abordagens de processamento são importantes diferenciar: \abrv[ESP -- Event Stream Processing]{(i) ESP (Event Stream Processing, ou, Processamento de Fluxo de Eventos)} e \abrv[CEP -- Complex Event Processing]{(ii) CEP (Complex Event Processing, ou, Processamento Complexo de Eventos)}. A primeira, costuma-se focar principalmente com questões de baixo nível, relacionadas a como processar eventos em tempo real atendendo requisitos de escalabilidade, tolerância a falha, confiabilidade, etc \cite{ConfluentArticle}.

Na segunda abordagem, os streams são utilizados para criar uma nuvem de eventos, parcialmente ordenados por tempo e causalidade, conceito conhecido como \abrv[POSET -- Partially ordered set of events]{POSET (Partially ordered set of events)} \cite{Complexevents}. Sendo assim, normalmente, visa-se trabalhar questões de alto nível, utilizando posets para a criação de padrões de eventos, envolvendo seus relacionamentos, estrutura interna, etc. Com esse conjunto de informações é possível compor eventos complexos, os quais podem ser consultados continuadamente \cite{ConfluentArticle}. 

Portanto, nessa monografia, será utilizada a primeira abordagem (ESP) para processamento de eventos, por ser mais adequada ao objetivo proposto no Cap. \ref{chapter1}. Em ESPs, algumas plataformas processam os streams continuadamente conforme são recebidos pelo sistema; paradigma conhecido como One at time. Quando um evento é processado com sucesso, há a possibilidade de emitir uma notificação sobre isso, a qual é custosa por ser necessário rastreá-lo até o término de seu processamento \cite{Datatorrent}. 

Outra alternativa, é a de realizar o processamento em micro-batchs (pequenos lotes) de eventos, tendo com uma das vantagens poder realizar operações em pequenos blocos de dados, em vez de individualmente, ao custo de introduzir um pequeno atraso no processo \cite{Datatorrent}.

\subsection{Metodologia de escolha das plataformas para processamento de fluxo de eventos em tempo real}
\label{metodologyOfChoice}

Inicialmente, foi realizada uma pesquisa com a palavra chave "stream processing" e analisados os primeiros artigos (na ordem do resultado e que citavam mais de uma ferramenta) em busca de citações a ferramentas (open source) de processamento de streams em tempo real. Com base nesse resultado, os nomes das plataformas encontradas foram utilizados para buscar a quantidade de referências no Google Scholar (site para publicação de artigos acadêmicos) e Stack Over Flow (site para discussão de assuntos relacionados a desenvolvimento de software). Na Tab. \ref{tab:tableNumberOfMentions}, constam os resultados desse procedimento.

A plataforma Esper mencionada em \cite{InfoQArticle} e \cite{ConfluentArticle}. foi descartada do processo de escolha por ser um componente para CEP, normalmente integrado a ferramentas ESPs, fugindo do escopo desse trabalho \cite{EsperTech}. Sendo assim, continuando a análise, foram também consideradas as características das opções restantes, estudadas superficialmente nos parágrafos seguintes.

\bigskip
\bigskip
\bigskip
\bigskip
\bigskip
\bigskip

\begin{table}[!htb]	
	\center
	{
		\begin{tabular}{c|c|c|c|c}
			\hline
            {\bf Referência} & {\bf Spark Streaming} & {\bf Storm} & {\bf Flink} & {\bf Samza}\\
			\hline
			\cite{InfoQArticle} & 1 & 1 & 1 & 1\\
			\cite{ConfluentArticle} & 1 & 0 & 0 & 1\\
			\cite{DzoneArticle} & 1 & 1 & 0 & 1\\
			\cite{Datatorrent} & 1 & 1 & 0 & 0\\
			\cite{CakeSolutionsArticle} & 1 & 1 & 0 & 0\\			
			\cite{VenturebeatArticle} & 1 & 1 & 0 & 0\\
			\cite{BravenewgeekArticle} & 1 & 0 & 0 & 1\\
			\cite{GoogleScholar} & 806 & 766 & 171 & 283\\
			\cite{StackOverFlow} & 1541 & 538 & 567 & 87\\
			\hline
			{\bf Total de citações} & 2354 & 1311 & 739 & 374\\
			\hline
		\end{tabular}
	}
	% Título de tabelas sempre aparecem antes da tabela
	\caption{Quantidade de citações a plataformas ESPs por referência}
	\label{tab:tableNumberOfMentions}
	{Fonte: Elaboração própria}	
\end{table}

\paragraph{Apache Spark Streaming.} Apache Spark Streaming é uma extensão do Spark para processamento de stream, em near real time. Além disso, o Spark possui outros módulos para consultas a banco de dados, grafos, e aprendizado de máquina \cite{Spark}. O processamento de stream é realizado em micro-batching pelo Spark Streaming (um de seus módulos), utilizando uma abstração conhecida como \abrv[DStream -- Discretized Stream]{DStream}, a qual é composta por uma sequências de \abrv[RDD -- Resilient Distributed Dataset]{RDDs (Resilient Distributed Dataset)}, ilustrada na Fig. \ref{fig:dstream}. RDDs abstraem uma coleção de dados distribuídos e manipulados em paralelo. No Spark, também é possível utilizar processamento em batch, independente do módulo que está sendo utilizado \cite{LearningSpark}.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.8\textwidth]{Imagens/dstreamImage.png}
  \caption{Ilustração de um DStream}
  {Fonte:} \cite{Spark}
  \label{fig:dstream}
\end{figure}

\paragraph{Apache Storm.} Apache Storm é um plataforma para processamento de stream em tempo real. Ao contrário do Spark, tem "one at time" como modelo de processamento de dados. As streams são compostas por tuplas (unidade básica de dados, as quais processadas numa abstração conhecida como Topologia, ilustarda na Fig. \ref{fig:stormTopology}, composta por um \abrv[DAG -- Directed Acyclic Graph]{DAG (Directed Acyclic Graph \cite{GraphTeory})} de spouts e bolts, respectivamente, resposáveis por emitir streams de dados e processá-los \cite{LearningStorm}. 

\bigskip
\bigskip

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/stormTopologyImage.png}
  \caption{Exemplo de uma topologia do Storm}
  {Fonte:} \cite{LearningStorm}
  \label{fig:stormTopology}
\end{figure}

\paragraph{Apache Flink.} Apache Flink é um plataforma muito semelhante ao Spark. Com suporte a processamento de streams, os quais são definidos por uma abstração conhecida como DataStream, ilustrada na Fig. \ref{fig:flinkProcessingImage}. O processamento em batch é realizado em cima de DataSets, semalhantes as RDDs do Spak. Da mesma forma como o DStream é definido como uma série de RDDS, o DataStream é composto por uma sequência de DataSets. A abstração Sink é utilizada para armazenar e retornar DataSets. Há suporte também para CEP, processamento em grafos, aprendizado de máquina e consulta a banco de dados \cite{Flink}.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.8\textwidth]{Imagens/flinkProcessingImage.png}
  \caption{Exemplo de fluxo de processamento no Flink}
  {Fonte:} \cite{FlinkOverView}
  \label{fig:flinkProcessingImage}
\end{figure}

\paragraph{Apache Samza.} Apache Samza é outra plataforma para processamento de stream em tempo real, utilizando como abstração base o conceito de mensagem (identificadas por offsets, ids), em vez de tupla, ou, DStream. Os streams são separados em partições contendo mensagens ordenadas (acessadas apenas no modo de leitura), sendo processados por jobs responsáveis por ler e emir fluxos. Assim como o Spark, há suporte para processamento em batch, o qual é realizado processando sequências de streams.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.6\textwidth]{Imagens/samzaPartitionsImage.png}
  \caption{Exemplo de um stream particionado}
  {Fonte:} \cite{Samza}
  \label{fig:samzaPartitions}
\end{figure}

Após essas breves descrições das plataformas para processamento de eventos em tempo real e da análise realizada do número de citações, o Spark Streaming e Storm foram escolhidas como ferramentas para o trabalho desenvolvido no Cap. \cite{chapter3}. Isso, porque ambas possuem maior número de citações nas fontes pesquisadas, e consequentemente, comunidades maiores, o que pode favorecer melhor documentação, suporte e continuidade desses projetos. 

Sendo assim, como Flink e Samza são semelhantes ao Spark o único critério de diferenciação, superfecialmente, é o tamanho da comunidade e a popularidade que o Spark tem. No demais, o Storm se diferencia dos demais com o conceito de topologia, interessante de ser estudado. Devido a isso, na seção seguinte o Spark e Storm são estudados mais aprofundamente.

\section{Apache Spark}

Apache Spark é uma plataforma de computação em cluster (unidade lógica composta por um conjunto de computadores conectados entre si através da Rede, permitindo compartilhamento de recursos \cite{Techopedia}), com suporte a consultas a banco de dados, processamento de streaming (em near real time), grafos, e aprendizado de máquina. Tendo Java, Python e Scala como linguagens de programação suportadas \cite{Spark}.

A arquitetura do Spark, conforme a Fig. \ref{fig:sparkStack}, é composta por uma pilha integrando os seguintes componentes, a serem explicados posteriormente: Spark SQL, Spark Streaming, MLib, GraphX, Spark Core e Administradores de Cluster (Yarn \cite{Yarn} e Apache Mesos \cite{Mesos}). Tal estrutura visa ser de fácil manutenção, teste, deploy, e permitir aumento de perfomance do núcleo impactando seus demais componentes.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/sparkStackImage.png}
  \caption{Ilustração da arquitetura em pilha do Apache Spark}
  {Fonte:} \cite{LearningSpark}
  \label{fig:sparkStack}
\end{figure}

\subsection{Módulos do Apache Spark}

O Spark Core é o principal módulo do Apache Spark, responsável principalmente pelo gerenciamento de memória,  tasks (conceito explicado em \ref{taskSection}) e tolerância a falha. Ainda nele, a abstração conhecida como  \abrv[RDD -- Resilient Distributed Dataset]{RDD (Resilient Distributed Dataset)} é definida, cujo papel é o de representar uma coleção de dados distribuídos e manipulados em paralelo. Os RDDs em Java são representados pela classe JavaRDD, criados através da classe JavaSparkContext (contexto da aplicação), que é instanciada recebendo como parâmetro um objeto SparkConf, contendo informações relacionadas ao nome da aplicação e o endereço em que ela será executada, pondendo ser local, ou, em cluster.

Outro módulo é o Spark Streaming, o qual realiza o processamento de dados em stream. Os streams são representados por uma sequência de RDDs, conhecida como \abrv[DStream -- Discretized Stream]{DStream}. O contexto de um streaming (JavaStreamingContext) é criado usando as configurações da aplicação e o intervalo no qual cada DStream será definido. Após isso, tais streams são atribuídos a um objeto da classe JavaReceiverInputDStream.

Além dos módulos detalhados nos parágrafos anteriores, é relevante mencionar o (i) Spark SQL, responsável por trabalhar com dados estruturados; o (ii) MLib, relacionado ao aprendizado de máquina, contendo algorítimos tais como o de classificação, regressão, "clusterização", filtragem colaborativa, avaliação de modelo e importação de dados; e o (ii) GraphX, que é composto por uma biblioteca para manipulação de grafos e computações paralelas. Por fim, o Spark também possui um administrador de cluster padrão, conhecido como Standalone Scheduler \cite{Spark}, tendo também suporte ao YARN e Apache Mesos.

\subsection{Composição Interna do Apache Spark}
\label{sparkInternalsSection}

Nesta seção é explicada a composição interna do Apache Spark, expondo os componentes que permitem a execução distribuída de uma aplicação spark, tais como o Driver Program, Spark Context, Worker Node e Executor.
Também são explicados os conceitos sobre as operações de Transformação e Ação, e os relativos a Job, Stage, Task e Partição RDD.

\subsubsection{Modelo de execução das aplicações Spark}
\label{sparkModelSection}

O modelo de execução das aplicações Spark é definido pela relação composta por um driver, SparkContext, executors e tasks, conforme ilustrado na Fig. \ref{fig:sparkComponents}. Nessa interação, as aplicações Spark funcionam num Worker Node (qualquer nó capaz de de executar código de aplicação localmente ou num cluster) como uma espécie de Driver, o qual executa operações paralelas e define dados distribuídos sobre um cluster. Além disso, o driver é responsável por encapsular a função principal da aplicação e prover acesso ao Spark, utilizando o objeto da classe SparkContext. Tal objeto é usado também para representar uma conexão com um cluster e construir RDDs \cite{LearningSpark}. 

Após a construção de um RDD, é possível executar operações sobre ele. Essas operações são realizadas por executors, processos administrados por uma aplicação Spark (cada aplicação tem os seus próprios executors); nos quais há espaços reservados para execução de tasks (conceito explicado na seção \ref{taskSection}). Há dois tipos de operações: (i) Ações (actions) e (ii) Transformações (transformations) \cite{Spark}. O primeiro tipo retorna um resultado ao driver após computações sobre um conjunto de dados de acordo com uma função. Sendo o segundo, responsável por gerar novos conjuntos de dados (RDDs) através, também, de funções especificadas \cite{LearningSpark}. 

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/sparkComponentsImage.png}
  \caption{Componentes do Spark para execução distribuída}
  {Fonte:} \cite{LearningSpark}
  \label{fig:sparkComponents}
\end{figure}

\subsubsection{Job}
\label{jobSection}

A abstração do conceito de  job está relacionada com o de action. Mais especificamente, um job é o conjunto de estágios (stages) resultantes de uma
determinada  action. Por exemplo, quando o método count() é invocado (para realizar a contagem de elementos dentro de um RDD), cria­-se um job composto por um ou mais estágios. Os jobs, por padrão, são escalonados para serem executados em ordem \abrv[FIFO -- First In, First Out]{FIFO (First In, First Out)}. Além disso, o scheduler do Spark é responsável por criar um plano de execução física, o qual é utilizado para calcular as RDDs necessárias para executar a ação invocada \cite{LearningSpark}, \cite{SparkInternals}.

Em tal plano de execução física, defini­-se basicamente um grafo acíclico dirigido \abrv[DAG -- Directed Acyclic Graph]{DAG (Directed Acyclic Graph \cite{GraphTeory})}, relacionando as dependências existentes entre os RDDs, numa espécie de linhagem de execução (lineage). Após isso, a partir da última RDD do último estágio, cada dependência é calculada, de trás para frente, para que posteriormente os RDDs dependentes possam ser calculados, até que todas as actions necessárias tenham terminado \cite{LearningSpark}, \cite{SparkInternals}.

\subsubsection{Stage}
\label{stageSection}

A divisão de um job resulta no conceito de stage, ou seja, jobs são compostos por um ou mais stages, os quais têm  tasks a serem executadas. Nem sempre a relação entre  stages e RDDs será de 1:1. No mais simples dos casos, para cada RDD há um stage, sendo possível nos mais complexos haver mais de um stage gerado por um único RDD \cite{LearningSpark}, \cite{SparkInternals}.

Por exemplo, havendo três RDDs no grafo RDD, não necessariamente haverão três stages. Um dos motivos para isso acontecer é quando ocorre pipelining, situação na qual é possível reduzir o número de  stages, calculando os valores de alguns  RDDs  utilizando unicamente informac?o?es de seus antecessores, sem movimentação de dados de outros RDDs \cite{LearningSpark}, \cite{SparkInternals}.

Além da possibilidade do número de stages ser reduzido, o contrário acontece quando fronteiras entre  stages são definidas. Tal situação ocorre porque algumas transformações, como a reduceByKey (função de agregação por chaves), podem resultar em reparticionamento do conjunto de dados para a computação de alguma saída, exigindo dados de outros RDDs para a formação de um único RDD. Assim, em cada fronteira entre os  stages, tasks do estágio antecessor são responsáveis por escrever dados para o disco e buscar tasks no estágio sucessor, através da rede, resultando em um processo custoso, conhecido como shuffling \cite{LearningSpark}, \cite{SparkInternals}.

O número de partições entre o estágio predecessor e sucessor podem ser diferentes, sendo possível customizá-lo, embora isso não seja recomendável. Tal aconselhamento é devido ao fato de que se o número for modificado para uma pequena quantidade de partições, tais podem sofrer com tasks overloaded (sobrecarregadas), do contrário, havendo partições em excesso, resultará em um alto número de shuffling entre elas \cite{LearningSpark}, \cite{SparkInternals}.

Em resumo, shuffling sempre acontecerá quando for necessário obter dados de outras partições para computar um resultado, sendo esse um procedimento custoso devido ao número de operações de entrada e saída envolvendo: leitura/escrita em disco e transferência de dados através da rede. No entanto, se necessário, o Spark provê uma forma eficiente para reparticionamento, conhecida como coalesce(), a qual reduz o número de partições sem causar movimentação de dados \cite{LearningSpark}.

\subsubsection{Task}
\label{taskSection}

A abstração do conceito task  está relacionada principalmente com stage, pois, uma vez que os estágios são definidos, tasks são criadas e enviadas para um scheduler interno. Além disso, tasks estão muito próximas da definição de partition, pelo fato de ser necessária a existe?ncia de uma task para cada partition, para executar as computações necessárias sobre ela \cite{LearningSpark}.

Portanto, o número de tasks em um stage é definido pelo número de partições existentes no RDD antecessor. E, por outro lado, o número de partições em um determinado RDD é igual ao número de partições existentes no RDD do qual ele depende. No entanto, há exceções em caso de (i) coalescing, processo o qual permite criar um RDD com menos partições que seu antecessor; (ii) união, sendo o número de partições resultado da soma do número de partições predecessoras; (iii) produto cartesiano, produto dos predecessores \cite{LearningSpark}.

Cada  task internamente é divida em algumas etapas, sendo elas: (i) carregamento dos dados de entrada, sejam eles provenientes de unidades de armazenamento (caso o RDD seja um RDD de entrada de dados), de um RDD existente (armazenado em memória cache), ou, de saídas de outro RDD (processo de shuffling), (ii) processamento da operação necessária para computação do RDD representado por ela, (iii) escrever a saída para o processo de shuffling, para um armazenamento externo ou para o driver (caso seja o RDD final de uma ação). Em resumo, uma  task é responsável por coletar os dados de entrada, processá-­los e, por fim, gerar uma saída \cite{LearningSpark}.

\section{Apache Storm}

Apache Storm \cite{Storm} é uma plataforma de computação em cluster, com suporte a processamento de stream  de dados em tempo real. O Storm requer pouca manutenção, é de fácil de administração e tem suporte a qualquer linguagem de programação (que leia e escreva fluxos de dados padronizados), posto que em seu núcleo é utilizado o Apache Thrift \cite{Thrift} (framework que permite o desenvolvimento de serviços multilinguagem) para definir e submeter topologias (conceito definido em \ref{topologySection}) \cite{Storm}.

A arquitetura do Storm, conforme a Fig. \ref{fig:stormComponents}, é composta pelos seguintes componentes, a serem explicados posteriormente: Nimbus, ZooKeeper e Supervisor Node.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{Imagens/stormComponentsImage.png}
  \caption{Ilustração da arquitetura do Apache Storm}
  {Fonte:} \cite{LearningStorm}
  \label{fig:stormComponents}
\end{figure}

\subsection{Composição interna do Apache Storm}

\subsubsection{Nimbus}
\label{nimbusSection}

Nimbus é o processo master executado num cluster Storm, tendo ele uma única instância e sendo responsável por distribuir o código da aplicação (job) para os nós workers (computadores que compõem o cluster), aos quais são atribuídas tasks (parte de um job). As tasks são monitoradas pelo Supervisor node, sendo reiniciadas em caso de falha e quando requisitado. Ainda, caso um Supervisor node falhe continuadamente, o job é atribuído para outro nó worker \cite{LearningStorm}, \cite{StormPython}. 

O Nimbus utiliza um protocolo de comunicação sem estado (Stateless protocol), ou seja, cada requisição é tratada individualmente, sem relação com a anterior, não armazenando dados ou estado \cite{Techopedia}, sendo assim todos os seus dados são armazenados no ZooKeeper (definido em \ref{zookeeperSection}). Além disso ele é projetado para ser fail­-fast, ou seja, em caso de falha é rapidamente reiniciado, sem afetar as  tasks em execução nos nós  workers \cite{LearningStorm}, \cite{StormPython}.

\subsubsection{Supervisor node}
\label{supervisorSection}

Cada Supervisor node é responsável pela administração de um determinado nó do cluster Storm; gerenciando o ciclo de vida de processos workers relacionados a execução das tasks (partes de uma topologia) atribuídas a ele próprio. Os workers quando em execução emitem "sinais de vida" (heartbeats), possibilitando o supervisor detectar e reiniciar caso não estejam respondendo. E, assim como o Nimbus, um Supervisor node é projetado para ser  fail­-fast \cite{LearningStorm}, \cite{StormPython}.

\subsubsection{ZooKeeper}
\label{zookeeperSection}

O ZooKeeper Cluster \cite{ZooKeeper} é responsável por coordenar processos, informações compartilhadas, tasks submetidas ao Storm e estados associados ao cluster, num contexto distribuído e de forma confiável, sendo possível aumentar o nível de confiabilidade tendo mais de um servidor ZooKeeper. O ZooKeeper atua também como o intermediário da comunicação entre Nimbus e os Supervisor nodes, pois eles não se comunicam diretamente entre si, ambos também podem ser finalizados sem afetar o cluster, visto que todos os seus dados, são armazenados no ZooKeeper, como já mencionado anteriormente \cite{LearningStorm}, \cite{StormPython}.

\subsubsection{Topologia}
\label{topologySection}

A topologia Storm, ilustrada na Fig. \ref{fig:stormTopology}, é uma abstração que define um grafo acíclico dirigido \abrv[DAG -- Directed Acyclic Graph]{DAG (Directed Acyclic Graph \cite{GraphTeory})}, utilizado para computações de streams. Cada nó desse grafo é responsável por executar algum processamento, enviando seu resultado para o próximo nó do fluxo. Após definida a topologia, ela pode ser executada localmente ou submetida a um cluster \cite{LearningStorm}, \cite{StormPython}.

Stream é um dos componentes da topologia Storm, definido como uma sequência de tuplas independentes e imutáveis, fornecidas por um ou mais spout e processadas por um ou mais bolt. Cada stream tem o seu respectivo ID, que é utilizado pelos bolts para consumirem e produzirem as tuplas. As tuplas compõem uma unidade básica de dados, suportando os tipos de dados (serializáveis) no qual a topologia está sendo desenvolvida.

\paragraph{Bolt.} Os bolts são unidades de processamento de dados (streams), sendo eles responsáveis por executar transformações simples sobre as tuplas, as quais são combinadas com outras formando complexas transformações. Também, eles podem ser inscritos para receberem informações tanto de outros bolts, quanto dos spouts, além de produzirem streams como saída \cite{LearningStorm}.

Tais streams são declarados, emitidos para outro bolt e processados, respectivamente, pelos métodos declareStream, emit e execute. As tuplas não precisam ser processadas imediatamente quando chegam a um bolt, pois, talvez haja a necessidade de aguardar para executar alguma operação de junção (join) com outra tupla, por exemplo. Também, inúmeros métodos definidos pela interface Tuple podem recuperar meta­dados associados com a tupla recebida via execute \cite{LearningStorm}.

Por exemplo, se um ID de mensagem está associado com uma tupla, o método execute deverá publicar um evento ack, ou, fail, para o bolt, caso contrário o Storm não tem como saber se uma tupla foi processada. Sendo assim, o bolt envia automaticamente uma mensagem de confirmação após o término da execução do método execute e, em caso de um evento de falha, é lançada uma exceção \cite{LearningStorm}.

Além disso, num contexto distribuído, a topologia pode ser serializada e submetida, via Nimbus, para um cluster. No qual, será processada por worker nodes. Nesse contexto, o método prepare pode ser utilizado para assegurar que o bolt está, após a desserialização, configurado corretamente para executar as tuplas \cite{LearningStorm}, \cite{StormPython}.

\paragraph{Spout.} Na topologia Storm um  Spout é o responsável pelo fornecimento de tuplas, as quais são lidas e escritas por ele utilizando uma fonte externa de dados \cite{LearningStorm}. 

As tuplas emitidas por um  spout são rastreadas pelo Storm até terminarem o processamento, sendo emitida uma confirmação ao término dele. Tal procedimento somente ocorre se um ID de mensagem foi gerado na emissão da tupla e, caso esse ID tenha sido definido como nulo, o rastreamento não irá acontecer \cite{LearningStorm}.

Outra opção é definir um  time­out a topologia, sendo dessa forma enviada uma mensagem de falha para o  spout,  caso a tupla não seja processada dentro do tempo estipulado. Sendo necessário, novamente, definir um ID de mensagem. O custo dessa confirmação do processamento ter sido executado com sucesso é a pequena perda de performance, que pode ser relevante em determinados contextos; sendo assim é possível ignorar a emissão de IDs de mensagens
\cite{LearningStorm}.

Os principais métodos de um spout são: o open(), executado quando o spout é inicializado, sendo nele definida a lógica para acesso a dados de fontes externas; o de declaração de stream (declareStream) e o de processamento de próxima tupla (nextTuple), que ocorre se houver confirmação de sucesso da tupla anterior (ack), sendo o método fail chamado pelo Storm caso isso não ocorra \cite{LearningStorm}.

Por fim, nenhum dos métodos utilizados para a construção de um  spout devem ser bloqueante, pois o Storm executa todos os métodos numa mesma  thread. Além disso, todo  spout tem um buffer interno para o rastreamento dos status das tuplas emitidas, as quais são mantidas nele até uma confirmação de processamento concluído com sucesso (ack) ou falha (fail); não sendo inseridas novas tuplas (via nextTuple) no buffer quando ele esá cheio \cite{LearningStorm}.
